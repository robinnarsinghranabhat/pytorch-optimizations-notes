{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b962b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "=============\n",
      "Profiling torch.square\n",
      "=============\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::square         0.00%      18.000us        99.97%        2.502s        2.502s       0.000us         0.00%     975.000us     975.000us             1  \n",
      "                                              aten::pow         0.03%     654.000us        99.97%        2.502s        2.502s     975.000us       100.00%     975.000us     975.000us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     975.000us       100.00%     975.000us     975.000us             1  \n",
      "                                      aten::result_type         0.00%       3.000us         0.00%       3.000us       3.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                               aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel        99.94%        2.501s        99.94%        2.501s        2.501s       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize         0.03%     789.000us         0.03%     789.000us     789.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 2.503s\n",
      "Self CUDA time total: 975.000us\n",
      "\n",
      "=============\n",
      "Profiling a * a\n",
      "=============\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul         8.56%     356.000us        78.40%       3.260ms       3.260ms     978.000us       100.00%     978.000us     978.000us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     978.000us       100.00%     978.000us     978.000us             1  \n",
      "                                       cudaLaunchKernel        69.84%       2.904ms        69.84%       2.904ms       2.904ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        21.60%     898.000us        21.60%     898.000us     898.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 4.158ms\n",
      "Self CUDA time total: 978.000us\n",
      "\n",
      "=============\n",
      "Profiling a ** 2\n",
      "=============\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::pow         6.26%     210.000us        72.76%       2.441ms       2.441ms     975.000us       100.00%     975.000us     975.000us             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     975.000us       100.00%     975.000us     975.000us             1  \n",
      "                                      aten::result_type         0.06%       2.000us         0.06%       2.000us       2.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                               aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel        66.44%       2.229ms        66.44%       2.229ms       2.229ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        27.24%     914.000us        27.24%     914.000us     914.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.355ms\n",
      "Self CUDA time total: 975.000us\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1., 2., 3.])\n",
    "\n",
    "print(torch.square(a))\n",
    "print(a ** 2)\n",
    "print(a * a)\n",
    "\n",
    "def time_pytorch_function(func, input):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)\n",
    "\n",
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "def square_2(a):\n",
    "    return a * a\n",
    "\n",
    "def square_3(a):\n",
    "    return a ** 2\n",
    "\n",
    "time_pytorch_function(torch.square, b)\n",
    "time_pytorch_function(square_2, b)\n",
    "time_pytorch_function(square_3, b)\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")\n",
    "\n",
    "# Now profile each function using pytorch profiler\n",
    "with torch.profiler.profile() as prof:\n",
    "    torch.square(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    square_2(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    square_3(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4d142a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch.cuda' from '/home/usd.local/robin.ranabhat/myenv/lib64/python3.6/site-packages/torch/cuda/__init__.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c416bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b22674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "148a90dc",
   "metadata": {},
   "source": [
    "##  Extras : Memory and Streams\n",
    "https://docs.pytorch.org/docs/stable/notes/cuda.html#memory-management\n",
    "\n",
    "As you run the program below, observe `watch -n 0.1 nvidia-smi`. \n",
    "if benchmark_sequential allocates around X MB of memory, while benchmark_parallel allocates around twice of this.\n",
    "This is because, `benchmark_with_streams` has two independent  allocation of `A` and `B` Tensor at same time.\n",
    "In First program, `benchmark_sequential` first allocates memory for `A`, runs some computation. And while allocationg memory for `B`, utilizes the caching feature of pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# setup\n",
    "device = 'cuda:0'\n",
    "model = models.resnet18().to(device)\n",
    "data = torch.randn(64, 3, 224, 224, device=device)\n",
    "target = torch.randint(0, 1000, (64,), device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "nb_iters = 20\n",
    "warmup_iters = 10\n",
    "for i in range(nb_iters):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # start profiling after 10 warmup iterations\n",
    "    if i == warmup_iters: torch.cuda.cudart().cudaProfilerStart()\n",
    "\n",
    "    # push range for current iteration\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_push(\"iteration{}\".format(i))\n",
    "\n",
    "    # push range for forward\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_push(\"forward\")\n",
    "    output = model(data)\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_push(\"backward\")\n",
    "    loss.backward()\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_push(\"opt.step()\")\n",
    "    optimizer.step()\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "    # pop iteration range\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "torch.cuda.cudart().cudaProfilerStop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47df3c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE SEQUENCIAL BENCHMARKING : The GPU USAGE :  157.81\n",
      "Starting GPU Usage :  211.81\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device not ready\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# --- 5. Run and Compare ---\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEFORE SEQUENCIAL BENCHMARKING : The GPU USAGE : \u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu_mem())\n\u001b[0;32m--> 156\u001b[0m sequential_time \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScenario 1 (Sequential): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequential_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms per run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()   \n",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m, in \u001b[0;36mbenchmark_sequential\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(M, M, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m end_cpu_gpu\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m---> 55\u001b[0m total_elapsed_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mstart_cpu_gpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melapsed_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_cpu_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(M, M, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m C \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(A, A)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/streams.py:213\u001b[0m, in \u001b[0;36mEvent.elapsed_time\u001b[0;34m(self, end_event)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21melapsed_time\u001b[39m(\u001b[38;5;28mself\u001b[39m, end_event):\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the time elapsed.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    Time reported in milliseconds after the event was recorded and\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    before the end_event was recorded.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melapsed_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_event\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device not ready\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gpu_mem():\n",
    "    \"\"\"\n",
    "    Returns the current GPU memory usage in MB\n",
    "    ( Equivalent to usage shown in nvidia-smi )\n",
    "    \"\"\"\n",
    "    mem = torch.cuda.mem_get_info()[1] / (1024 ** 2) - (torch.cuda.mem_get_info()[0] / 1024 ** 2)\n",
    "    return round(mem, 2)\n",
    "\n",
    "# --- 2. Define Benchmark Parameters ---\n",
    "n_warmup = 5\n",
    "n_runs = 10\n",
    "\n",
    "M = 9000\n",
    "\n",
    "# --- 3. Scenario 1: Without Streams (Sequential) ---\n",
    "def benchmark_sequential():\n",
    "\n",
    "\n",
    "    # Warm-up runs\n",
    "    for _ in range(n_warmup):\n",
    "        A = torch.randn(M, M, device='cuda')\n",
    "        B = torch.randn(M, M, device='cuda')\n",
    "        C = torch.mm(A, A)\n",
    "        C = torch.softmax(C, dim=1)\n",
    "        C = torch.tanh(C)\n",
    "        D = torch.mm(B, B)\n",
    "        D = torch.softmax(D, dim=1)\n",
    "        D = torch.tanh(D)\n",
    "        # C.to('cpu')\n",
    "        # D.to('cpu')\n",
    "    \n",
    "    # Clear the cache to measure correct memory usage\n",
    "    del A, B, C, D\n",
    "    torch.cuda.empty_cache()\n",
    "    # Wait for the GPU to finish all queued work\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"Starting GPU Usage : \", gpu_mem())\n",
    "    # Timing\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Timing for CPU to GPU transfer\n",
    "    start_cpu_gpu = torch.cuda.Event(enable_timing=True)\n",
    "    end_cpu_gpu = torch.cuda.Event(enable_timing=True)\n",
    "    total_elapsed_times = 0\n",
    "    \n",
    "    start_event.record()\n",
    "    for _ in range(n_runs):\n",
    "        start_cpu_gpu.record()\n",
    "        A = torch.randn(M, M, device='cuda')\n",
    "        end_cpu_gpu.record()\n",
    "        total_elapsed_times += start_cpu_gpu.elapsed_time(end_cpu_gpu)\n",
    "        B = torch.randn(M, M, device='cuda')\n",
    "        \n",
    "        C = torch.mm(A, A)\n",
    "        C = torch.softmax(C, dim=1)\n",
    "        C = torch.tanh(C)\n",
    "        D = torch.mm(B, B)\n",
    "        D = torch.softmax(D, dim=1)\n",
    "        D = torch.tanh(D)\n",
    "        # C.to('cpu')\n",
    "        # D.to('cpu')\n",
    "        \n",
    "        \n",
    "    end_event.record()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    # Wait for the GPU to finish all queued work\n",
    "    torch.cuda.synchronize()\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    print(\"ELAPSED TIME FOR DATA TRANSFER\", total_elapsed_times / n_runs)\n",
    "    print(\"Total GPU Usage : \", gpu_mem())\n",
    "    return elapsed_time_ms / n_runs\n",
    "\n",
    "# --- 4. Scenario 2: With Streams (Concurrent) ---\n",
    "def benchmark_with_streams():\n",
    "\n",
    "    s1 = torch.cuda.Stream()\n",
    "    s2 = torch.cuda.Stream()\n",
    "\n",
    "    # Warm-up runs\n",
    "    for _ in range(n_warmup):\n",
    "        with torch.cuda.stream(s1):\n",
    "            A = torch.randn(M, M, device='cuda')\n",
    "            C = torch.mm(A, A)\n",
    "            C = torch.softmax(C, dim=1)\n",
    "            C = torch.tanh(C)\n",
    "            # C.to('cpu')\n",
    "        with torch.cuda.stream(s2):\n",
    "            B = torch.randn(M, M, device='cuda')\n",
    "            D = torch.mm(B, B)\n",
    "            D = torch.softmax(D, dim=1)\n",
    "            D = torch.tanh(D)\n",
    "            # D.to('cpu')\n",
    "\n",
    "    # Clear the cache to measure correct memory usage\n",
    "    del A, B, C, D\n",
    "    torch.cuda.empty_cache()\n",
    "    # Wait for the GPU to finish all queued work\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "\n",
    "    print(\"Starting GPU Usage : \", gpu_mem())\n",
    "    # Timing\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Timing for CPU to GPU transfer\n",
    "    start_cpu_gpu = torch.cuda.Event(enable_timing=True)\n",
    "    end_cpu_gpu = torch.cuda.Event(enable_timing=True)\n",
    "    total_elapsed_times = 0\n",
    "\n",
    "    \n",
    "    start_event.record()\n",
    "    print(\"Starting stream benchmark...\")\n",
    "    for i in range(n_runs):\n",
    "        with torch.cuda.stream(s1):\n",
    "            start_cpu_gpu.record()\n",
    "            A = torch.randn(M, M, device='cuda')\n",
    "            end_cpu_gpu.record()\n",
    "            total_elapsed_times += start_cpu_gpu.elapsed_time(end_cpu_gpu)\n",
    "            C = torch.mm(A, A)\n",
    "            C = torch.softmax(C, dim=1)\n",
    "            C = torch.tanh(C)\n",
    "            # C.to('cpu')\n",
    "        with torch.cuda.stream(s2):\n",
    "            B = torch.randn(M, M, device='cuda')\n",
    "            D = torch.mm(B, B)\n",
    "            D = torch.softmax(D, dim=1)\n",
    "            D = torch.tanh(D)\n",
    "            # D.to('cpu')\n",
    "    end_event.record()\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    # Wait for the GPU to finish all queued work\n",
    "    torch.cuda.synchronize()\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "    \n",
    "\n",
    "    print(\"ELAPSED TIME FOR DATA TRANSFER\", total_elapsed_times / n_runs)\n",
    "    \n",
    "    print(\"Final GPU Usage : \", gpu_mem())\n",
    "    return elapsed_time_ms / n_runs\n",
    "\n",
    "\n",
    "# --- 5. Run and Compare ---\n",
    "\n",
    "print(\"BEFORE SEQUENCIAL BENCHMARKING : The GPU USAGE : \", gpu_mem())\n",
    "sequential_time = benchmark_sequential()\n",
    "print(f\"Scenario 1 (Sequential): {sequential_time:.3f} ms per run\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()   \n",
    "print(\"BEFORE STREAM BENCHMARKING : The GPU USAGE : \", gpu_mem()) \n",
    "streams_time = benchmark_with_streams()\n",
    "print(f\"Scenario 2 (With Streams): {streams_time:.3f} ms per run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f769086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac59076",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE STREAM OPS  : Cache cleared. The GPU USAGE :  271.81\n",
    "Stream Warm-up complete. GPU USAGE :  1655.81\n",
    "Starting Fresh Timing with cache cleared GPU USAGE :  1079.81\n",
    "Starting stream benchmark...\n",
    "END Fresh Timing with current GPU USAGE :  1655.81\n",
    "Scenario 2 (With Streams): 0.770 ms per run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd1d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "## still in cache, so remove them and observe memory usage in nvidia-smi` \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a57ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86b5c4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9709.81"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77089adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10822.9375"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).total_memory / 1024 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94561ed6",
   "metadata": {},
   "source": [
    "## Profiling Default Stream in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6729a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling sequential execution...\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                          ProfilerStep*         0.27%      19.492ms        99.99%        7.342s        1.835s       0.000us         0.00%        6.675s        1.669s             4  \n",
      "                                            aten::copy_         8.87%     651.076ms        99.58%        7.312s     121.867ms        1.656s        24.81%        1.656s      27.600ms            60  \n",
      "                                               aten::to         0.02%       1.786ms        90.81%        6.668s     166.695ms       0.000us         0.00%        1.656s      41.399ms            40  \n",
      "                                         aten::_to_copy         0.06%       4.041ms        90.78%        6.666s     166.651ms       0.000us         0.00%        1.656s      41.399ms            40  \n",
      "                                        cudaMemcpyAsync        86.39%        6.343s        86.39%        6.343s     158.581ms       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                  cudaStreamSynchronize         4.33%     317.738ms         4.33%     317.738ms       7.943ms       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                               aten::mm         0.02%       1.486ms         0.04%       2.654ms      26.540us        4.962s        74.34%        4.962s      49.622ms           100  \n",
      "                                       cudaLaunchKernel         0.02%       1.591ms         0.02%       1.591ms       8.839us       0.000us         0.00%       0.000us       0.000us           180  \n",
      "                                             aten::add_         0.01%     664.000us         0.02%       1.217ms      15.213us      56.385ms         0.84%      56.385ms     704.812us            80  \n",
      "                                            aten::zeros         0.01%     883.000us         0.01%     910.000us     227.500us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 7.343s\n",
      "Self CUDA time total: 6.675s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Profiling with CUDA Streams\n",
    "### https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "### Personal :: https://gemini.google.com/app/061daa91e4502050\n",
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "ITERS = 5\n",
    "            \n",
    "# --- Profiling Setup ---\n",
    "size = 7000\n",
    "log_dir = \"./log\"\n",
    "\n",
    "\n",
    "A = torch.ones(size, size, device=\"cpu\", pin_memory=True)\n",
    "collected_data = [torch.empty(size, size, device=\"cpu\", pin_memory=True) for _ in range(ITERS)]\n",
    "\n",
    "def sequential_data_processing(size):\n",
    "    for i in range(ITERS):\n",
    "        A_gpu = A.to(\"cuda\")\n",
    "        C_gpu = torch.mm(A_gpu, A_gpu)\n",
    "        for _ in range(4):\n",
    "            C_gpu += torch.mm(A_gpu, A_gpu)\n",
    "        C_gpu = C_gpu.to(\"cpu\")\n",
    "        collected_data[i].copy_(C_gpu, non_blocking=False)\n",
    "\n",
    "\n",
    "# --- Profile the Sequential Case ---\n",
    "print(\"Profiling sequential execution...\")\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/sequential'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof_sequential:\n",
    "    for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "        sequential_data_processing(size)\n",
    "        prof_sequential.step() # Mark the end of an iteration\n",
    "\n",
    "print(prof_sequential.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef79aa",
   "metadata": {},
   "source": [
    "## Profiling Multiple Streams in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61af9436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling execution with streams...\n",
      "Profiling complete. Traces saved in './log' directory.\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                  cudaDeviceSynchronize        99.82%       10.008s        99.82%       10.008s       10.008s       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                          ProfilerStep*         0.11%      11.252ms         0.18%      17.723ms       4.431ms       0.000us         0.00%        6.018s        1.504s             4  \n",
      "                                               aten::to         0.01%     844.000us         0.03%       3.090ms     154.500us       0.000us         0.00%     394.427ms      19.721ms            20  \n",
      "                                         aten::_to_copy         0.02%       1.751ms         0.02%       2.246ms     112.300us       0.000us         0.00%     394.427ms      19.721ms            20  \n",
      "                                               aten::mm         0.01%       1.206ms         0.02%       1.996ms      19.960us        5.202s        43.30%        5.202s      52.024ms           100  \n",
      "                                       cudaLaunchKernel         0.01%       1.264ms         0.01%       1.264ms       7.022us       0.000us         0.00%       0.000us       0.000us           180  \n",
      "                                             aten::add_         0.01%     580.000us         0.01%       1.089ms      13.613us      95.653ms         0.80%      95.653ms       1.196ms            80  \n",
      "                                            aten::copy_         0.00%     274.000us         0.01%     600.000us      15.000us     719.889ms         5.99%     719.889ms      17.997ms            40  \n",
      "                                            aten::zeros         0.00%     371.000us         0.00%     385.000us      96.250us       0.000us         0.00%       0.000us       0.000us             4  \n",
      "                                        cudaMemcpyAsync         0.00%     326.000us         0.00%     326.000us       8.150us       0.000us         0.00%       0.000us       0.000us            40  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 10.027s\n",
      "Self CUDA time total: 12.015s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "# --- Profiling Setup ---\n",
    "print(\"Profiling execution with streams...\")\n",
    "size = 7000\n",
    "log_dir = \"./log\"\n",
    "# --- Profile the Streams Case ---\n",
    "ITERS = 5\n",
    "collected_data = [torch.empty(size, size, device=\"cpu\", pin_memory=True) for _ in range(ITERS)]\n",
    "\n",
    "A = torch.ones(size, size, device=\"cpu\", pin_memory=True) # Use pinned memory for faster async copies\n",
    "## SETUP STREAMS\n",
    "streams = []\n",
    "for i in range(ITERS):\n",
    "    streams.append(torch.cuda.Stream())\n",
    "\n",
    "\n",
    "def streamed_data_processing(size):\n",
    "    # 1. QUEUE all operations without waiting\n",
    "    for i in range(ITERS):\n",
    "        with torch.cuda.stream(streams[i]):\n",
    "            A_gpu = A.to(\"cuda\", non_blocking=True)\n",
    "            C_gpu = torch.mm(A_gpu, A_gpu)\n",
    "            for _ in range(4):\n",
    "                C_gpu += torch.mm(A_gpu, A_gpu)\n",
    "            collected_data[i].copy_(C_gpu, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/with_streams'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof_stream:\n",
    "    for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "        streamed_data_processing(size)\n",
    "        prof_stream.step() # Mark the end of an iteration\n",
    "\n",
    "print(f\"Profiling complete. Traces saved in '{log_dir}' directory.\")\n",
    "print(prof_stream.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d075a5ef-ccd6-45c2-be59-9ab31d4e7e24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2a3bf8-1147-4d8b-a473-cf70bbf73b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec83ae9-6805-4fdd-8923-e44e15115904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "# --- Profiling Setup ---\n",
    "print(\"Profiling execution with streams...\")\n",
    "size = 7000\n",
    "log_dir = \"./log\"\n",
    "# --- Profile the Streams Case ---\n",
    "ITERS = 5\n",
    "collected_data = [torch.empty(size, size, device=\"cpu\", pin_memory=True) for _ in range(ITERS)]\n",
    "\n",
    "A = torch.ones(size, size, device=\"cpu\", pin_memory=True) # Use pinned memory for faster async copies\n",
    "## SETUP STREAMS\n",
    "streams = []\n",
    "for i in range(ITERS):\n",
    "    streams.append(torch.cuda.Stream())\n",
    "\n",
    "\n",
    "def streamed_data_processing(size):\n",
    "\n",
    "    # Allocate input tensors\n",
    "    N = 1000000000  # 3 billion elements (# H100 94 GB)\n",
    "    A1 = torch.randn(N, device=\"cuda\")\n",
    "    B1 = torch.randn(N, device=\"cuda\")\n",
    "    C1 = torch.empty_like(A1)\n",
    "\n",
    "    A2 = torch.randn(N, device=\"cuda\")\n",
    "    B2 = torch.randn(N, device=\"cuda\")\n",
    "    C2 = torch.empty_like(A2)\n",
    "\n",
    "    # Create two CUDA streams\n",
    "    stream1 = torch.cuda.Stream()\n",
    "    stream2 = torch.cuda.Stream()\n",
    "\n",
    "    # Launch vector addition in stream1\n",
    "    with torch.cuda.stream(stream1):\n",
    "        C1.copy_(A1 + B1)  # PyTorch kernel for element-wise addition\n",
    "\n",
    "    # Launch vector addition in stream2\n",
    "    with torch.cuda.stream(stream2):\n",
    "        C2.copy_(A2 + B2)\n",
    "\n",
    "# Wait for both kernels to finish\n",
    "torch.cuda.synchronize()\n",
    "\n",
    "print(\"Both vector additions ran on separate streams!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "    on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/with_streams'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof_stream:\n",
    "    for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "        streamed_data_processing(size)\n",
    "        prof_stream.step() # Mark the end of an iteration\n",
    "\n",
    "print(f\"Profiling complete. Traces saved in '{log_dir}' directory.\")\n",
    "print(prof_stream.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7ef83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.profiler\n",
    "\n",
    "# # --- Profiling Setup ---\n",
    "# print(\"Profiling execution with streams...\")\n",
    "# size = 1000\n",
    "# log_dir = \"./log\"\n",
    "# # --- Profile the Streams Case ---\n",
    "# ITERS = 5\n",
    "# collected_data = [torch.empty(size, size, device=\"cpu\", pin_memory=True) for _ in range(ITERS)]\n",
    "\n",
    "# A = torch.ones(size, size, device=\"cpu\", pin_memory=True) # Use pinned memory for faster async copies\n",
    "# ## SETUP STREAMS\n",
    "# streams = []\n",
    "# for i in range(ITERS):\n",
    "#     streams.append(torch.cuda.Stream())\n",
    "\n",
    "\n",
    "# def streamed_data_processing(size):\n",
    "#     # 1. QUEUE all operations without waiting\n",
    "#     for i in range(ITERS):\n",
    "#         with torch.cuda.stream(streams[i]):\n",
    "#             A_gpu = A.to(\"cuda\", non_blocking=True)\n",
    "#             C_gpu = torch.mm(A_gpu, A_gpu)\n",
    "#             for _ in range(10):\n",
    "#                 C_gpu += torch.mm(A_gpu, A_gpu)\n",
    "#             collected_data[i].copy_(C_gpu, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "# with torch.profiler.profile(\n",
    "#     activities=[\n",
    "#         torch.profiler.ProfilerActivity.CPU,\n",
    "#         torch.profiler.ProfilerActivity.CUDA,\n",
    "#     ],\n",
    "#     schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "#     # on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/with_streams'),\n",
    "#     record_shapes=True,\n",
    "#     with_stack=True\n",
    "# ) as prof_stream:\n",
    "#     for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "#         streamed_data_processing(size)\n",
    "#         prof_stream.step() # Mark the end of an iteration\n",
    "\n",
    "# print(f\"Profiling complete. Traces saved in '{log_dir}' directory.\")\n",
    "# print(prof_stream.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8a9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones(size, size, device=\"cpu\")\n",
    "C = torch.mm(A, A)\n",
    "# collected_data[i] = C_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbfe4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_data = [torch.empty(size, size, device=\"cpu\") for _ in range(ITERS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a8d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 6.8664e-44,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        ...,\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_data[0].copy_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade73cb5",
   "metadata": {},
   "source": [
    "## RANDOM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2048c51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling execution with streams...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-21 21:31:39 3142936:3142936 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling complete. Traces saved in './log' directory.\n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    ProfilerStep*         2.69%       9.899ms        99.38%     365.838ms      91.460ms       0.000us         0.00%     338.026ms      84.507ms             4  \n",
      "                                         aten::to         0.14%     527.000us        64.01%     235.613ms       2.945ms       0.000us         0.00%     274.106ms       3.426ms            80  \n",
      "                                   aten::_to_copy         0.23%     833.000us        63.90%     235.216ms       2.940ms       0.000us         0.00%     278.071ms       3.476ms            80  \n",
      "                                      aten::copy_         0.29%       1.077ms        63.43%     233.495ms       2.919ms     278.071ms        40.94%     278.071ms       3.476ms            80  \n",
      "                                  cudaMemcpyAsync        63.14%     232.418ms        63.14%     232.418ms       2.905ms       0.000us         0.00%       0.000us       0.000us            80  \n",
      "                                       aten::ones         0.09%     349.000us        31.95%     117.605ms       2.940ms       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                      aten::fill_        31.80%     117.048ms        31.80%     117.048ms       2.926ms       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                         aten::mm         0.47%       1.742ms         0.69%       2.529ms      63.225us      59.955ms         8.83%      59.955ms       1.499ms            40  \n",
      "                            cudaDeviceSynchronize         0.62%       2.272ms         0.62%       2.272ms       2.272ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                              aten::empty_strided         0.21%     780.000us         0.24%     888.000us      11.100us       0.000us         0.00%       0.000us       0.000us            80  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 368.110ms\n",
      "Self CUDA time total: 679.190ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-21 21:31:39 3142936:3142936 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-07-21 21:31:39 3142936:3142936 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "def func():\n",
    "    A = torch.ones(size, size, device=\"cpu\")\n",
    "    A = A.to(\"cuda\", non_blocking=True)\n",
    "    C = torch.mm(A, A)\n",
    "    C = C.to(\"cpu\", non_blocking=True)\n",
    "\n",
    "streams = []\n",
    "for i in range(10):\n",
    "    streams.append(torch.cuda.Stream()) \n",
    "\n",
    "def run_with_streams(size):\n",
    "    \"\"\"Runs creation and matmul concurrently on two separate streams.\"\"\"\n",
    "    for i in range(10):\n",
    "        with torch.cuda.stream(streams[i]):\n",
    "            func()\n",
    "\n",
    "# --- Profiling Setup ---\n",
    "size = 2000\n",
    "log_dir = \"./log\"\n",
    "# --- Profile the Streams Case ---\n",
    "print(\"Profiling execution with streams...\")\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/with_streams'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof_stream:\n",
    "    for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "        run_with_streams(size)\n",
    "        prof_stream.step() # Mark the end of an iteration\n",
    "\n",
    "print(f\"Profiling complete. Traces saved in '{log_dir}' directory.\")\n",
    "print(prof_stream.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d626a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a89497f",
   "metadata": {},
   "source": [
    "## Nvidia Nsight Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dde7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Allocate input tensors\n",
    "# N = 1000000000  # 1 billion elements (adjusted for compatibility)\n",
    "# A1 = torch.randn(N, device=\"cuda\")\n",
    "# B1 = torch.randn(N, device=\"cuda\")\n",
    "# C1 = torch.empty_like(A1)\n",
    "\n",
    "# A2 = torch.randn(N, device=\"cuda\")\n",
    "# B2 = torch.randn(N, device=\"cuda\")\n",
    "# C2 = torch.empty_like(A2)\n",
    "\n",
    "# # Create two CUDA streams\n",
    "# stream1 = torch.cuda.Stream()\n",
    "# stream2 = torch.cuda.Stream()\n",
    "\n",
    "# # Setup for profiling\n",
    "# nb_iters = 10\n",
    "# warmup_iters = 3\n",
    "\n",
    "# for i in range(nb_iters):\n",
    "#     # Start profiling after warmup iterations\n",
    "#     if i == warmup_iters: \n",
    "#         torch.cuda.cudart().cudaProfilerStart()\n",
    "    \n",
    "#     # Push range for current iteration\n",
    "#     if i >= warmup_iters: \n",
    "#         torch.cuda.nvtx.range_push(\"iteration{}\".format(i))\n",
    "    \n",
    "#     # Push range for stream1 operation\n",
    "#     if i >= warmup_iters: \n",
    "#         torch.cuda.nvtx.range_push(\"stream1_vector_add\")\n",
    "    \n",
    "#     # Launch vector addition in stream1\n",
    "#     with torch.cuda.stream(stream1):\n",
    "#         C1.copy_(A1 + B1)  # PyTorch kernel for element-wise addition\n",
    "    \n",
    "#     if i >= warmup_iters: \n",
    "#         torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "#     # Push range for stream2 operation\n",
    "#     if i >= warmup_iters: \n",
    "#         torch.cuda.nvtx.range_push(\"stream2_vector_add\")\n",
    "    \n",
    "#     # Launch vector addition in stream2\n",
    "#     with torch.cuda.stream(stream2):\n",
    "#         C2.copy_(A2 + B2)\n",
    "    \n",
    "#     if i >= warmup_iters: \n",
    "#         torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "#     # Push range for synchronization\n",
    "#     if i >= warmup_iters: \n",
    "#         torch.cuda.nvtx.range_push(\"synchronize\")\n",
    "    \n",
    "#     # Wait for both kernels to finish\n",
    "#     torch.cuda.synchronize()\n",
    "    \n",
    "#     if i >= warmup_iters: \n",
    "#         torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "#     # Pop iteration range\n",
    "#     if i >= warmup_iters: \n",
    "#         torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# # Stop profiling\n",
    "# torch.cuda.cudart().cudaProfilerStop()\n",
    "\n",
    "# print(\"Both vector additions ran on separate streams!\")\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "input = []\n",
    "batch_size = 64\n",
    "shape = (batch_size, 256, 256)\n",
    "for i in range (0, 4):\n",
    "    input.append(torch.randn(shape))\n",
    "input = [in_.pin_memory() for in_ in input]\n",
    "\n",
    "s1 = torch.cuda.Stream()\n",
    "s2 = torch.cuda.Stream()\n",
    "s3 = torch.cuda.Stream()\n",
    "s4 = torch.cuda.Stream()\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "cpu_device = torch.device('cpu')\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "torch.cuda.cudart().cudaProfilerStart()\n",
    "\n",
    "# Stream 1 operations\n",
    "torch.cuda.nvtx.range_push(\"stream1_processing\")\n",
    "with torch.cuda.stream(s1):\n",
    "    torch.cuda.nvtx.range_push(\"stream1_h2d_copy\")\n",
    "    curr_batch = input[0].to(device,non_blocking = True)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    torch.cuda.nvtx.range_push(\"stream1_exp_ops\")\n",
    "    curr_batch = torch.exp (curr_batch)\n",
    "    curr_batch = torch.exp (curr_batch)\n",
    "    curr_batch = torch.matmul (curr_batch, curr_batch)\n",
    "    curr_batch = torch.matmul (curr_batch, curr_batch)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    torch.cuda.nvtx.range_push(\"stream1_d2h_copy\")\n",
    "    input[0].copy_(curr_batch,non_blocking = True)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# Stream 2 operations\n",
    "torch.cuda.nvtx.range_push(\"stream2_processing\")\n",
    "with torch.cuda.stream(s2):\n",
    "    torch.cuda.nvtx.range_push(\"stream2_h2d_copy\")\n",
    "    curr_batch = input[1].to(device,non_blocking = True)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    torch.cuda.nvtx.range_push(\"stream2_exp_ops\")\n",
    "    curr_batch = torch.exp (curr_batch)\n",
    "    curr_batch = torch.exp (curr_batch)\n",
    "    curr_batch = torch.matmul (curr_batch, curr_batch)\n",
    "    curr_batch = torch.matmul (curr_batch, curr_batch)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    torch.cuda.nvtx.range_push(\"stream2_d2h_copy\")\n",
    "    input[1].copy_(curr_batch,non_blocking = True)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# Stream 3 operations\n",
    "torch.cuda.nvtx.range_push(\"stream3_processing\")\n",
    "with torch.cuda.stream(s3):\n",
    "    torch.cuda.nvtx.range_push(\"stream3_h2d_copy\")\n",
    "    curr_batch = input[2].to(device,non_blocking = True)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    torch.cuda.nvtx.range_push(\"stream3_exp_ops\")\n",
    "    curr_batch = torch.exp (curr_batch)\n",
    "    curr_batch = torch.exp (curr_batch)\n",
    "    curr_batch = torch.matmul (curr_batch, curr_batch)\n",
    "    curr_batch = torch.matmul (curr_batch, curr_batch)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    torch.cuda.nvtx.range_push(\"stream3_d2h_copy\")\n",
    "    input[2].copy_(curr_batch,non_blocking = True)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# Stream 4 operations\n",
    "torch.cuda.nvtx.range_push(\"stream4_processing\")\n",
    "with torch.cuda.stream(s4):\n",
    "    torch.cuda.nvtx.range_push(\"stream4_h2d_copy\")\n",
    "    curr_batch = input[3].to(device,non_blocking = True)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    torch.cuda.nvtx.range_push(\"stream4_exp_ops\")\n",
    "    curr_batch = torch.exp (curr_batch)\n",
    "    curr_batch = torch.exp (curr_batch)\n",
    "    curr_batch = torch.matmul (curr_batch, curr_batch)\n",
    "    curr_batch = torch.matmul (curr_batch, curr_batch)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    torch.cuda.nvtx.range_push(\"stream4_d2h_copy\")\n",
    "    input[3].copy_(curr_batch,non_blocking = True)\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n",
    "torch.cuda.nvtx.range_push(\"final_sync\")\n",
    "torch.cuda.synchronize()\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n",
    "torch.cuda.cudart().cudaProfilerStop()\n",
    "\n",
    "\n",
    "# nsys profile --force-overwrite true  -w true -t cuda,nvtx,osrt,cudnn,cublas -s cpu --capture-range=cudaProfilerApi --cudabacktrace=true -x true -o test python stream_Add.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344ca8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamed execution profiling...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ae90db3c92fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mITERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcollected_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ae90db3c92fc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mITERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0min_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mcollected_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mITERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Setup ---\n",
    "print(\"Streamed execution profiling...\")\n",
    "size = 256\n",
    "ITERS = 5\n",
    "\n",
    "shape = (64, size, size)\n",
    "# Use pinned memory for faster async copies\n",
    "# A = torch.ones(64, size, size, device=\"cpu\", pin_memory=True)\n",
    "input = []\n",
    "for i in range (ITERS):\n",
    "    input.append(torch.randn(shape, device=\"cpu\"))\n",
    "input = [in_.pin_memory() for in_ in input]\n",
    "\n",
    "collected_data = [torch.empty(64, size, size, device=\"cpu\", pin_memory=True) for _ in range(ITERS)]\n",
    "\n",
    "# Setup CUDA streams\n",
    "streams = []\n",
    "for i in range(ITERS):\n",
    "    streams.append(torch.cuda.Stream())\n",
    "\n",
    "def streamed_data_processing():\n",
    "    \"\"\"Streamed processing - pipeline host-to-GPU, kernel, GPU-to-host operations\"\"\"\n",
    "    torch.cuda.synchronize()\n",
    "    # Queue all operations across streams without waiting\n",
    "    for i in range(ITERS):\n",
    "        with torch.cuda.stream(streams[i]):\n",
    "            # Push range for current iteration\n",
    "            torch.cuda.nvtx.range_push(f\"stream_{i}_iteration\")\n",
    "            \n",
    "            # Host to GPU transfer (non-blocking)\n",
    "            torch.cuda.nvtx.range_push(f\"stream_{i}_host_to_gpu\")\n",
    "            A_gpu = input[i].to(\"cuda\", non_blocking=True)\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "            \n",
    "            # Kernel computation\n",
    "            torch.cuda.nvtx.range_push(f\"stream_{i}_kernel_computation\")\n",
    "            C_gpu = torch.mm(A_gpu, A_gpu) + torch.mm(A_gpu, A_gpu) + torch.mm(A_gpu, A_gpu)\n",
    "            # for _ in range(10):\n",
    "            #     C_gpu += torch.mm(A_gpu, A_gpu)\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "            \n",
    "            # GPU to host transfer (non-blocking)\n",
    "            torch.cuda.nvtx.range_push(f\"stream_{i}_gpu_to_host\")\n",
    "            collected_data[i].copy_(C_gpu, non_blocking=True)\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "            \n",
    "            # Pop iteration range\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    # Wait for all streams to complete\n",
    "    torch.cuda.nvtx.range_push(\"stream_synchronization\")\n",
    "    torch.cuda.synchronize()\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# Warmup iterations (not profiled)\n",
    "warmup_iters = 2\n",
    "print(\"Running warmup iterations...\")\n",
    "for _ in range(warmup_iters):\n",
    "    streamed_data_processing()\n",
    "\n",
    "# Start profiling\n",
    "print(\"Starting profiled iterations...\")\n",
    "torch.cuda.cudart().cudaProfilerStart()\n",
    "\n",
    "# Profiled iterations\n",
    "torch.cuda.nvtx.range_push(\"streamed_processing\")\n",
    "streamed_data_processing()\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# Stop profiling\n",
    "torch.cuda.cudart().cudaProfilerStop()\n",
    "\n",
    "print(\"Streamed profiling complete.\")\n",
    "print(\"Run with: nsys profile -w true -t cuda,nvtx,osrt,cudnn,cublas -s cpu --capture-range=cudaProfilerApi --stop-on-range-end=true --cudabacktrace=true -x true -o streamed_profile python streamed_main.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39ab30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fc3ab4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling complete! Trace saved to 'vector_add_trace.json'.\n",
      "Open chrome://tracing in your browser and load the file to view it.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "# Allocate input tensors\n",
    "N = 1000000000  # 3 billion elements\n",
    "A1 = torch.randn(N, device=\"cuda\")\n",
    "B1 = torch.randn(N, device=\"cuda\")\n",
    "C1 = torch.empty_like(A1)\n",
    "\n",
    "A2 = torch.randn(N, device=\"cuda\")\n",
    "B2 = torch.randn(N, device=\"cuda\")\n",
    "C2 = torch.empty_like(A2)\n",
    "\n",
    "# Create two CUDA streams\n",
    "stream1 = torch.cuda.Stream()\n",
    "stream2 = torch.cuda.Stream()\n",
    "\n",
    "# --- PyTorch Profiler ---\n",
    "# The profiler context manager wraps the code we want to analyze.\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    record_shapes=True # Optionally records tensor shapes\n",
    ") as prof:\n",
    "    # Launch vector addition in stream1\n",
    "    with torch.cuda.stream(stream1):\n",
    "        C1.copy_(A1 + B1)\n",
    "\n",
    "    # Launch vector addition in stream2\n",
    "    with torch.cuda.stream(stream2):\n",
    "        C2.copy_(A2 + B2)\n",
    "\n",
    "# Note: An explicit torch.cuda.synchronize() is not needed here because\n",
    "# the profiler context manager automatically synchronizes when it exits.\n",
    "\n",
    "# Export the trace to a JSON file\n",
    "prof.export_chrome_trace(\"vector_add_trace.json\")\n",
    "\n",
    "print(\"Profiling complete! Trace saved to 'vector_add_trace.json'.\")\n",
    "print(\"Open chrome://tracing in your browser and load the file to view it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc51e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
