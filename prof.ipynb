{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b962b108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "tensor([1., 4., 9.])\n",
      "=============\n",
      "Profiling torch.square\n",
      "=============\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                           aten::square         0.33%      11.000us        59.33%       2.000ms       2.000ms       0.000us         0.00%       1.458ms       1.458ms             1  \n",
      "                                              aten::pow        57.96%       1.954ms        59.00%       1.989ms       1.989ms       1.458ms       100.00%       1.458ms       1.458ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.458ms       100.00%       1.458ms       1.458ms             1  \n",
      "                                      aten::result_type         0.06%       2.000us         0.06%       2.000us       2.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                               aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel         0.98%      33.000us         0.98%      33.000us      33.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        40.67%       1.371ms        40.67%       1.371ms       1.371ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.371ms\n",
      "Self CUDA time total: 1.458ms\n",
      "\n",
      "=============\n",
      "Profiling a * a\n",
      "=============\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::mul        57.78%       1.958ms        58.63%       1.987ms       1.987ms       1.465ms       100.00%       1.465ms       1.465ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.465ms       100.00%       1.465ms       1.465ms             1  \n",
      "                                       cudaLaunchKernel         0.86%      29.000us         0.86%      29.000us      29.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        41.37%       1.402ms        41.37%       1.402ms       1.402ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.389ms\n",
      "Self CUDA time total: 1.465ms\n",
      "\n",
      "=============\n",
      "Profiling a ** 2\n",
      "=============\n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                              aten::pow        57.44%       1.908ms        58.13%       1.931ms       1.931ms       1.463ms       100.00%       1.463ms       1.463ms             1  \n",
      "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       1.463ms       100.00%       1.463ms       1.463ms             1  \n",
      "                                      aten::result_type         0.06%       2.000us         0.06%       2.000us       2.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                               aten::to         0.00%       0.000us         0.00%       0.000us       0.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                       cudaLaunchKernel         0.63%      21.000us         0.63%      21.000us      21.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                                  cudaDeviceSynchronize        41.87%       1.391ms        41.87%       1.391ms       1.391ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 3.322ms\n",
      "Self CUDA time total: 1.463ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n",
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n",
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-07-20 19:29:53 2545450:2545450 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1., 2., 3.])\n",
    "\n",
    "print(torch.square(a))\n",
    "print(a ** 2)\n",
    "print(a * a)\n",
    "\n",
    "def time_pytorch_function(func, input):\n",
    "    # CUDA IS ASYNC so can't use python time module\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Warmup\n",
    "    for _ in range(5):\n",
    "        func(input)\n",
    "\n",
    "    start.record()\n",
    "    func(input)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    return start.elapsed_time(end)\n",
    "\n",
    "b = torch.randn(10000, 10000).cuda()\n",
    "\n",
    "def square_2(a):\n",
    "    return a * a\n",
    "\n",
    "def square_3(a):\n",
    "    return a ** 2\n",
    "\n",
    "time_pytorch_function(torch.square, b)\n",
    "time_pytorch_function(square_2, b)\n",
    "time_pytorch_function(square_3, b)\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling torch.square\")\n",
    "print(\"=============\")\n",
    "\n",
    "# Now profile each function using pytorch profiler\n",
    "with torch.profiler.profile() as prof:\n",
    "    torch.square(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a * a\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    square_2(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))\n",
    "\n",
    "print(\"=============\")\n",
    "print(\"Profiling a ** 2\")\n",
    "print(\"=============\")\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    square_3(b)\n",
    "\n",
    "print(prof.key_averages().table(sort_by=\"cuda_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d142a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c416bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b22674",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "148a90dc",
   "metadata": {},
   "source": [
    "##  Extras : Memory and Streams\n",
    "https://docs.pytorch.org/docs/stable/notes/cuda.html#memory-management\n",
    "\n",
    "As you run the program below, observe `watch -n 0.1 nvidia-smi`. \n",
    "if benchmark_sequential allocates around X MB of memory, while benchmark_parallel allocates around twice of this.\n",
    "This is because, `benchmark_with_streams` has two independent  allocation of `A` and `B` Tensor at same time.\n",
    "In First program, `benchmark_sequential` first allocates memory for `A`, runs some computation. And while allocationg memory for `B`, utilizes the caching feature of pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3f36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# setup\n",
    "device = 'cuda:0'\n",
    "model = models.resnet18().to(device)\n",
    "data = torch.randn(64, 3, 224, 224, device=device)\n",
    "target = torch.randint(0, 1000, (64,), device=device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "nb_iters = 20\n",
    "warmup_iters = 10\n",
    "for i in range(nb_iters):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # start profiling after 10 warmup iterations\n",
    "    if i == warmup_iters: torch.cuda.cudart().cudaProfilerStart()\n",
    "\n",
    "    # push range for current iteration\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_push(\"iteration{}\".format(i))\n",
    "\n",
    "    # push range for forward\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_push(\"forward\")\n",
    "    output = model(data)\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_push(\"backward\")\n",
    "    loss.backward()\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_push(\"opt.step()\")\n",
    "    optimizer.step()\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "    # pop iteration range\n",
    "    if i >= warmup_iters: torch.cuda.nvtx.range_pop()\n",
    "\n",
    "torch.cuda.cudart().cudaProfilerStop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47df3c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE SEQUENCIAL BENCHMARKING : The GPU USAGE :  157.81\n",
      "Starting GPU Usage :  211.81\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device not ready\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 156\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# --- 5. Run and Compare ---\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBEFORE SEQUENCIAL BENCHMARKING : The GPU USAGE : \u001b[39m\u001b[38;5;124m\"\u001b[39m, gpu_mem())\n\u001b[0;32m--> 156\u001b[0m sequential_time \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_sequential\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScenario 1 (Sequential): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msequential_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ms per run\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    160\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()   \n",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m, in \u001b[0;36mbenchmark_sequential\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(M, M, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m end_cpu_gpu\u001b[38;5;241m.\u001b[39mrecord()\n\u001b[0;32m---> 55\u001b[0m total_elapsed_times \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mstart_cpu_gpu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melapsed_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_cpu_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m B \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(M, M, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     58\u001b[0m C \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(A, A)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/streams.py:213\u001b[0m, in \u001b[0;36mEvent.elapsed_time\u001b[0;34m(self, end_event)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21melapsed_time\u001b[39m(\u001b[38;5;28mself\u001b[39m, end_event):\n\u001b[1;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Return the time elapsed.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \n\u001b[1;32m    210\u001b[0m \u001b[38;5;124;03m    Time reported in milliseconds after the event was recorded and\u001b[39;00m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;124;03m    before the end_event was recorded.\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43melapsed_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_event\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device not ready\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def gpu_mem():\n",
    "    \"\"\"\n",
    "    Returns the current GPU memory usage in MB\n",
    "    ( Equivalent to usage shown in nvidia-smi )\n",
    "    \"\"\"\n",
    "    mem = torch.cuda.mem_get_info()[1] / (1024 ** 2) - (torch.cuda.mem_get_info()[0] / 1024 ** 2)\n",
    "    return round(mem, 2)\n",
    "\n",
    "# --- 2. Define Benchmark Parameters ---\n",
    "n_warmup = 5\n",
    "n_runs = 10\n",
    "\n",
    "M = 9000\n",
    "\n",
    "# --- 3. Scenario 1: Without Streams (Sequential) ---\n",
    "def benchmark_sequential():\n",
    "\n",
    "\n",
    "    # Warm-up runs\n",
    "    for _ in range(n_warmup):\n",
    "        A = torch.randn(M, M, device='cuda')\n",
    "        B = torch.randn(M, M, device='cuda')\n",
    "        C = torch.mm(A, A)\n",
    "        C = torch.softmax(C, dim=1)\n",
    "        C = torch.tanh(C)\n",
    "        D = torch.mm(B, B)\n",
    "        D = torch.softmax(D, dim=1)\n",
    "        D = torch.tanh(D)\n",
    "        # C.to('cpu')\n",
    "        # D.to('cpu')\n",
    "    \n",
    "    # Clear the cache to measure correct memory usage\n",
    "    del A, B, C, D\n",
    "    torch.cuda.empty_cache()\n",
    "    # Wait for the GPU to finish all queued work\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"Starting GPU Usage : \", gpu_mem())\n",
    "    # Timing\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Timing for CPU to GPU transfer\n",
    "    start_cpu_gpu = torch.cuda.Event(enable_timing=True)\n",
    "    end_cpu_gpu = torch.cuda.Event(enable_timing=True)\n",
    "    total_elapsed_times = 0\n",
    "    \n",
    "    start_event.record()\n",
    "    for _ in range(n_runs):\n",
    "        start_cpu_gpu.record()\n",
    "        A = torch.randn(M, M, device='cuda')\n",
    "        end_cpu_gpu.record()\n",
    "        total_elapsed_times += start_cpu_gpu.elapsed_time(end_cpu_gpu)\n",
    "        B = torch.randn(M, M, device='cuda')\n",
    "        \n",
    "        C = torch.mm(A, A)\n",
    "        C = torch.softmax(C, dim=1)\n",
    "        C = torch.tanh(C)\n",
    "        D = torch.mm(B, B)\n",
    "        D = torch.softmax(D, dim=1)\n",
    "        D = torch.tanh(D)\n",
    "        # C.to('cpu')\n",
    "        # D.to('cpu')\n",
    "        \n",
    "        \n",
    "    end_event.record()\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    # Wait for the GPU to finish all queued work\n",
    "    torch.cuda.synchronize()\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "\n",
    "    print(\"ELAPSED TIME FOR DATA TRANSFER\", total_elapsed_times / n_runs)\n",
    "    print(\"Total GPU Usage : \", gpu_mem())\n",
    "    return elapsed_time_ms / n_runs\n",
    "\n",
    "# --- 4. Scenario 2: With Streams (Concurrent) ---\n",
    "def benchmark_with_streams():\n",
    "\n",
    "    s1 = torch.cuda.Stream()\n",
    "    s2 = torch.cuda.Stream()\n",
    "\n",
    "    # Warm-up runs\n",
    "    for _ in range(n_warmup):\n",
    "        with torch.cuda.stream(s1):\n",
    "            A = torch.randn(M, M, device='cuda')\n",
    "            C = torch.mm(A, A)\n",
    "            C = torch.softmax(C, dim=1)\n",
    "            C = torch.tanh(C)\n",
    "            # C.to('cpu')\n",
    "        with torch.cuda.stream(s2):\n",
    "            B = torch.randn(M, M, device='cuda')\n",
    "            D = torch.mm(B, B)\n",
    "            D = torch.softmax(D, dim=1)\n",
    "            D = torch.tanh(D)\n",
    "            # D.to('cpu')\n",
    "\n",
    "    # Clear the cache to measure correct memory usage\n",
    "    del A, B, C, D\n",
    "    torch.cuda.empty_cache()\n",
    "    # Wait for the GPU to finish all queued work\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "\n",
    "    print(\"Starting GPU Usage : \", gpu_mem())\n",
    "    # Timing\n",
    "    start_event = torch.cuda.Event(enable_timing=True)\n",
    "    end_event = torch.cuda.Event(enable_timing=True)\n",
    "\n",
    "    # Timing for CPU to GPU transfer\n",
    "    start_cpu_gpu = torch.cuda.Event(enable_timing=True)\n",
    "    end_cpu_gpu = torch.cuda.Event(enable_timing=True)\n",
    "    total_elapsed_times = 0\n",
    "\n",
    "    \n",
    "    start_event.record()\n",
    "    print(\"Starting stream benchmark...\")\n",
    "    for i in range(n_runs):\n",
    "        with torch.cuda.stream(s1):\n",
    "            start_cpu_gpu.record()\n",
    "            A = torch.randn(M, M, device='cuda')\n",
    "            end_cpu_gpu.record()\n",
    "            total_elapsed_times += start_cpu_gpu.elapsed_time(end_cpu_gpu)\n",
    "            C = torch.mm(A, A)\n",
    "            C = torch.softmax(C, dim=1)\n",
    "            C = torch.tanh(C)\n",
    "            # C.to('cpu')\n",
    "        with torch.cuda.stream(s2):\n",
    "            B = torch.randn(M, M, device='cuda')\n",
    "            D = torch.mm(B, B)\n",
    "            D = torch.softmax(D, dim=1)\n",
    "            D = torch.tanh(D)\n",
    "            # D.to('cpu')\n",
    "    end_event.record()\n",
    "    \n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    # Wait for the GPU to finish all queued work\n",
    "    torch.cuda.synchronize()\n",
    "    # Calculate elapsed time\n",
    "    elapsed_time_ms = start_event.elapsed_time(end_event)\n",
    "    \n",
    "\n",
    "    print(\"ELAPSED TIME FOR DATA TRANSFER\", total_elapsed_times / n_runs)\n",
    "    \n",
    "    print(\"Final GPU Usage : \", gpu_mem())\n",
    "    return elapsed_time_ms / n_runs\n",
    "\n",
    "\n",
    "# --- 5. Run and Compare ---\n",
    "\n",
    "print(\"BEFORE SEQUENCIAL BENCHMARKING : The GPU USAGE : \", gpu_mem())\n",
    "sequential_time = benchmark_sequential()\n",
    "print(f\"Scenario 1 (Sequential): {sequential_time:.3f} ms per run\")\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()   \n",
    "print(\"BEFORE STREAM BENCHMARKING : The GPU USAGE : \", gpu_mem()) \n",
    "streams_time = benchmark_with_streams()\n",
    "print(f\"Scenario 2 (With Streams): {streams_time:.3f} ms per run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f769086",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac59076",
   "metadata": {},
   "outputs": [],
   "source": [
    "BEFORE STREAM OPS  : Cache cleared. The GPU USAGE :  271.81\n",
    "Stream Warm-up complete. GPU USAGE :  1655.81\n",
    "Starting Fresh Timing with cache cleared GPU USAGE :  1079.81\n",
    "Starting stream benchmark...\n",
    "END Fresh Timing with current GPU USAGE :  1655.81\n",
    "Scenario 2 (With Streams): 0.770 ms per run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fd1d028",
   "metadata": {},
   "outputs": [],
   "source": [
    "## still in cache, so remove them and observe memory usage in nvidia-smi` \n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a57ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86b5c4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9709.81"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_mem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77089adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10822.9375"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_properties(0).total_memory / 1024 ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94561ed6",
   "metadata": {},
   "source": [
    "## Profiling Default Stream in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6729a72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling sequential execution...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-22 01:34:05 3377808:3377808 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    ProfilerStep*         0.26%       1.877ms       100.00%     717.268ms     179.317ms       0.000us         0.00%     703.321ms     175.830ms             4  \n",
      "                                      aten::copy_         0.11%     797.000us        99.39%     712.905ms      17.823ms     528.696ms        37.29%     528.696ms      13.217ms            40  \n",
      "                                  cudaMemcpyAsync        99.05%     710.478ms        99.05%     710.478ms      17.762ms       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                         aten::to         0.02%     145.000us        37.10%     266.126ms      13.306ms       0.000us         0.00%     260.428ms      13.021ms            20  \n",
      "                                   aten::_to_copy         0.05%     369.000us        37.08%     265.981ms      13.299ms       0.000us         0.00%     260.428ms      13.021ms            20  \n",
      "                                         aten::mm         0.17%       1.214ms         0.24%       1.691ms      84.550us     174.625ms        12.32%     174.625ms       8.731ms            20  \n",
      "                            cudaStreamSynchronize         0.23%       1.630ms         0.23%       1.630ms      40.750us       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                 cudaLaunchKernel         0.06%     422.000us         0.06%     422.000us      21.100us       0.000us         0.00%       0.000us       0.000us            20  \n",
      "                              aten::empty_strided         0.04%     281.000us         0.04%     281.000us      14.050us       0.000us         0.00%       0.000us       0.000us            20  \n",
      "    cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.01%      55.000us         0.01%      55.000us       2.750us       0.000us         0.00%       0.000us       0.000us            20  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 717.290ms\n",
      "Self CUDA time total: 1.418s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-22 01:34:05 3377808:3377808 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-07-22 01:34:05 3377808:3377808 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "## Profiling with CUDA Streams\n",
    "### https://docs.pytorch.org/tutorials/recipes/recipes/profiler_recipe.html\n",
    "### Personal :: https://gemini.google.com/app/061daa91e4502050\n",
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "ITERS = 5\n",
    "            \n",
    "# --- Profiling Setup ---\n",
    "size = 4000\n",
    "log_dir = \"./log\"\n",
    "\n",
    "\n",
    "A = torch.ones(size, size, device=\"cpu\")\n",
    "collected_data = [torch.empty(size, size, device=\"cpu\") for _ in range(ITERS)]\n",
    "\n",
    "def sequential_data_processing(size):\n",
    "    for i in range(ITERS):\n",
    "        A_gpu = A.to(\"cuda\")\n",
    "        C_gpu = torch.mm(A_gpu, A_gpu)\n",
    "        # for _ in range(10):\n",
    "        #     C_gpu += torch.mm(A_gpu, A_gpu)\n",
    "        # C = C.to(\"cpu\")\n",
    "        collected_data[i].copy_(C_gpu, non_blocking=False)\n",
    "\n",
    "\n",
    "# --- Profile the Sequential Case ---\n",
    "print(\"Profiling sequential execution...\")\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/sequential'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof_sequential:\n",
    "    for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "        sequential_data_processing(size)\n",
    "        prof_sequential.step() # Mark the end of an iteration\n",
    "\n",
    "print(prof_sequential.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ef79aa",
   "metadata": {},
   "source": [
    "## Profiling Multiple Streams in CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61af9436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling execution with streams...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-22 01:34:31 3377808:3377808 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling complete. Traces saved in './log' directory.\n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    ProfilerStep*         0.61%       4.612ms        99.99%     750.032ms     187.508ms       0.000us         0.00%     733.904ms     183.476ms             4  \n",
      "                                      aten::copy_         0.13%     984.000us        99.00%     742.600ms      18.565ms     559.661ms        38.09%     559.661ms      13.992ms            40  \n",
      "                                  cudaMemcpyAsync        98.87%     741.616ms        98.87%     741.616ms      18.540ms       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                         aten::to         0.02%     147.000us        37.44%     280.817ms      14.041ms       0.000us         0.00%     276.100ms      13.805ms            20  \n",
      "                                   aten::_to_copy         0.06%     449.000us        37.42%     280.670ms      14.034ms       0.000us         0.00%     276.100ms      13.805ms            20  \n",
      "                                         aten::mm         0.18%       1.368ms         0.25%       1.906ms      95.300us     174.243ms        11.86%     174.243ms       8.712ms            20  \n",
      "                                 cudaLaunchKernel         0.06%     448.000us         0.06%     448.000us      22.400us       0.000us         0.00%       0.000us       0.000us            20  \n",
      "                              aten::empty_strided         0.04%     318.000us         0.04%     318.000us      15.900us       0.000us         0.00%       0.000us       0.000us            20  \n",
      "    cudaOccupancyMaxActiveBlocksPerMultiprocessor         0.01%      90.000us         0.01%      90.000us       4.500us       0.000us         0.00%       0.000us       0.000us            20  \n",
      "                            cudaDeviceSynchronize         0.01%      44.000us         0.01%      44.000us      44.000us       0.000us         0.00%       0.000us       0.000us             1  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 750.076ms\n",
      "Self CUDA time total: 1.469s\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-22 01:34:31 3377808:3377808 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-07-22 01:34:31 3377808:3377808 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "# --- Profiling Setup ---\n",
    "print(\"Profiling execution with streams...\")\n",
    "size = 4000\n",
    "log_dir = \"./log\"\n",
    "# --- Profile the Streams Case ---\n",
    "ITERS = 5\n",
    "collected_data = [torch.empty(size, size, device=\"cpu\") for _ in range(ITERS)]\n",
    "\n",
    "A = torch.ones(size, size, device=\"cpu\") # Use pinned memory for faster async copies\n",
    "## SETUP STREAMS\n",
    "streams = []\n",
    "for i in range(ITERS):\n",
    "    streams.append(torch.cuda.Stream())\n",
    "\n",
    "\n",
    "def streamed_data_processing(size):\n",
    "    # 1. QUEUE all operations without waiting\n",
    "    for i in range(ITERS):\n",
    "        with torch.cuda.stream(streams[i]):\n",
    "            A_gpu = A.to(\"cuda\", non_blocking=True)\n",
    "            C_gpu = torch.mm(A_gpu, A_gpu)\n",
    "            # for _ in range(10):\n",
    "            #     C_gpu += torch.mm(A_gpu, A_gpu)\n",
    "            collected_data[i].copy_(C_gpu, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/with_streams'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof_stream:\n",
    "    for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "        streamed_data_processing(size)\n",
    "        prof_stream.step() # Mark the end of an iteration\n",
    "\n",
    "print(f\"Profiling complete. Traces saved in '{log_dir}' directory.\")\n",
    "print(prof_stream.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac7ef83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.profiler\n",
    "\n",
    "# # --- Profiling Setup ---\n",
    "# print(\"Profiling execution with streams...\")\n",
    "# size = 1000\n",
    "# log_dir = \"./log\"\n",
    "# # --- Profile the Streams Case ---\n",
    "# ITERS = 5\n",
    "# collected_data = [torch.empty(size, size, device=\"cpu\", pin_memory=True) for _ in range(ITERS)]\n",
    "\n",
    "# A = torch.ones(size, size, device=\"cpu\", pin_memory=True) # Use pinned memory for faster async copies\n",
    "# ## SETUP STREAMS\n",
    "# streams = []\n",
    "# for i in range(ITERS):\n",
    "#     streams.append(torch.cuda.Stream())\n",
    "\n",
    "\n",
    "# def streamed_data_processing(size):\n",
    "#     # 1. QUEUE all operations without waiting\n",
    "#     for i in range(ITERS):\n",
    "#         with torch.cuda.stream(streams[i]):\n",
    "#             A_gpu = A.to(\"cuda\", non_blocking=True)\n",
    "#             C_gpu = torch.mm(A_gpu, A_gpu)\n",
    "#             for _ in range(10):\n",
    "#                 C_gpu += torch.mm(A_gpu, A_gpu)\n",
    "#             collected_data[i].copy_(C_gpu, non_blocking=True)\n",
    "\n",
    "\n",
    "\n",
    "# with torch.profiler.profile(\n",
    "#     activities=[\n",
    "#         torch.profiler.ProfilerActivity.CPU,\n",
    "#         torch.profiler.ProfilerActivity.CUDA,\n",
    "#     ],\n",
    "#     schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "#     # on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/with_streams'),\n",
    "#     record_shapes=True,\n",
    "#     with_stack=True\n",
    "# ) as prof_stream:\n",
    "#     for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "#         streamed_data_processing(size)\n",
    "#         prof_stream.step() # Mark the end of an iteration\n",
    "\n",
    "# print(f\"Profiling complete. Traces saved in '{log_dir}' directory.\")\n",
    "# print(prof_stream.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c8a9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.ones(size, size, device=\"cpu\")\n",
    "C = torch.mm(A, A)\n",
    "# collected_data[i] = C_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbfe4cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "collected_data = [torch.empty(size, size, device=\"cpu\") for _ in range(ITERS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9a8d5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 6.8664e-44,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        ...,\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03],\n",
       "        [2.0000e+03, 2.0000e+03, 2.0000e+03,  ..., 2.0000e+03, 2.0000e+03,\n",
       "         2.0000e+03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collected_data[0].copy_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade73cb5",
   "metadata": {},
   "source": [
    "## RANDOM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2048c51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling execution with streams...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-21 21:31:39 3142936:3142936 ActivityProfilerController.cpp:314] Completed Stage: Warm Up\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profiling complete. Traces saved in './log' directory.\n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                             Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    ProfilerStep*         2.69%       9.899ms        99.38%     365.838ms      91.460ms       0.000us         0.00%     338.026ms      84.507ms             4  \n",
      "                                         aten::to         0.14%     527.000us        64.01%     235.613ms       2.945ms       0.000us         0.00%     274.106ms       3.426ms            80  \n",
      "                                   aten::_to_copy         0.23%     833.000us        63.90%     235.216ms       2.940ms       0.000us         0.00%     278.071ms       3.476ms            80  \n",
      "                                      aten::copy_         0.29%       1.077ms        63.43%     233.495ms       2.919ms     278.071ms        40.94%     278.071ms       3.476ms            80  \n",
      "                                  cudaMemcpyAsync        63.14%     232.418ms        63.14%     232.418ms       2.905ms       0.000us         0.00%       0.000us       0.000us            80  \n",
      "                                       aten::ones         0.09%     349.000us        31.95%     117.605ms       2.940ms       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                      aten::fill_        31.80%     117.048ms        31.80%     117.048ms       2.926ms       0.000us         0.00%       0.000us       0.000us            40  \n",
      "                                         aten::mm         0.47%       1.742ms         0.69%       2.529ms      63.225us      59.955ms         8.83%      59.955ms       1.499ms            40  \n",
      "                            cudaDeviceSynchronize         0.62%       2.272ms         0.62%       2.272ms       2.272ms       0.000us         0.00%       0.000us       0.000us             1  \n",
      "                              aten::empty_strided         0.21%     780.000us         0.24%     888.000us      11.100us       0.000us         0.00%       0.000us       0.000us            80  \n",
      "-------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 368.110ms\n",
      "Self CUDA time total: 679.190ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2025-07-21 21:31:39 3142936:3142936 ActivityProfilerController.cpp:320] Completed Stage: Collection\n",
      "STAGE:2025-07-21 21:31:39 3142936:3142936 ActivityProfilerController.cpp:324] Completed Stage: Post Processing\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.profiler\n",
    "\n",
    "def func():\n",
    "    A = torch.ones(size, size, device=\"cpu\")\n",
    "    A = A.to(\"cuda\", non_blocking=True)\n",
    "    C = torch.mm(A, A)\n",
    "    C = C.to(\"cpu\", non_blocking=True)\n",
    "\n",
    "streams = []\n",
    "for i in range(10):\n",
    "    streams.append(torch.cuda.Stream()) \n",
    "\n",
    "def run_with_streams(size):\n",
    "    \"\"\"Runs creation and matmul concurrently on two separate streams.\"\"\"\n",
    "    for i in range(10):\n",
    "        with torch.cuda.stream(streams[i]):\n",
    "            func()\n",
    "\n",
    "# --- Profiling Setup ---\n",
    "size = 2000\n",
    "log_dir = \"./log\"\n",
    "# --- Profile the Streams Case ---\n",
    "print(\"Profiling execution with streams...\")\n",
    "with torch.profiler.profile(\n",
    "    activities=[\n",
    "        torch.profiler.ProfilerActivity.CPU,\n",
    "        torch.profiler.ProfilerActivity.CUDA,\n",
    "    ],\n",
    "    schedule=torch.profiler.schedule(wait=2, warmup=4, active=4, repeat=1),\n",
    "    # on_trace_ready=torch.profiler.tensorboard_trace_handler(f'{log_dir}/with_streams'),\n",
    "    record_shapes=True,\n",
    "    with_stack=True\n",
    ") as prof_stream:\n",
    "    for _ in range(10): # 1 wait, 1 warmup, 2 active\n",
    "        run_with_streams(size)\n",
    "        prof_stream.step() # Mark the end of an iteration\n",
    "\n",
    "print(f\"Profiling complete. Traces saved in '{log_dir}' directory.\")\n",
    "print(prof_stream.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d626a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a89497f",
   "metadata": {},
   "source": [
    "## Nvidia Nsight Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "344ca8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streamed execution profiling...\n",
      "Running warmup iterations...\n",
      "Starting profiled iterations...\n",
      "Streamed profiling complete.\n",
      "Run with: nsys profile -w true -t cuda,nvtx,osrt,cudnn,cublas -s cpu --capture-range=cudaProfilerApi --stop-on-range-end=true --cudabacktrace=true -x true -o streamed_profile python streamed_main.py\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# --- Setup ---\n",
    "print(\"Streamed execution profiling...\")\n",
    "size = 1000\n",
    "ITERS = 5\n",
    "\n",
    "# Use pinned memory for faster async copies\n",
    "A = torch.ones(size, size, device=\"cpu\")\n",
    "collected_data = [torch.empty(size, size, device=\"cpu\") for _ in range(ITERS)]\n",
    "\n",
    "# Setup CUDA streams\n",
    "streams = []\n",
    "for i in range(ITERS):\n",
    "    streams.append(torch.cuda.Stream())\n",
    "\n",
    "def streamed_data_processing():\n",
    "    \"\"\"Streamed processing - pipeline host-to-GPU, kernel, GPU-to-host operations\"\"\"\n",
    "    \n",
    "    # Queue all operations across streams without waiting\n",
    "    for i in range(ITERS):\n",
    "        with torch.cuda.stream(streams[i]):\n",
    "            # Push range for current iteration\n",
    "            torch.cuda.nvtx.range_push(f\"stream_{i}_iteration\")\n",
    "            \n",
    "            # Host to GPU transfer (non-blocking)\n",
    "            torch.cuda.nvtx.range_push(f\"stream_{i}_host_to_gpu\")\n",
    "            A_gpu = A.to(\"cuda\", non_blocking=True)\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "            \n",
    "            # Kernel computation\n",
    "            torch.cuda.nvtx.range_push(f\"stream_{i}_kernel_computation\")\n",
    "            C_gpu = torch.mm(A_gpu, A_gpu)\n",
    "            # for _ in range(10):\n",
    "            #     C_gpu += torch.mm(A_gpu, A_gpu)\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "            \n",
    "            # GPU to host transfer (non-blocking)\n",
    "            torch.cuda.nvtx.range_push(f\"stream_{i}_gpu_to_host\")\n",
    "            collected_data[i].copy_(C_gpu, non_blocking=True)\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "            \n",
    "            # Pop iteration range\n",
    "            torch.cuda.nvtx.range_pop()\n",
    "    \n",
    "    # Wait for all streams to complete\n",
    "    torch.cuda.nvtx.range_push(\"stream_synchronization\")\n",
    "    for stream in streams:\n",
    "        stream.synchronize()\n",
    "    torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# Warmup iterations (not profiled)\n",
    "warmup_iters = 2\n",
    "print(\"Running warmup iterations...\")\n",
    "for _ in range(warmup_iters):\n",
    "    streamed_data_processing()\n",
    "\n",
    "# Start profiling\n",
    "print(\"Starting profiled iterations...\")\n",
    "torch.cuda.cudart().cudaProfilerStart()\n",
    "\n",
    "# Profiled iterations\n",
    "torch.cuda.nvtx.range_push(\"streamed_processing\")\n",
    "streamed_data_processing()\n",
    "torch.cuda.nvtx.range_pop()\n",
    "\n",
    "# Stop profiling\n",
    "torch.cuda.cudart().cudaProfilerStop()\n",
    "\n",
    "print(\"Streamed profiling complete.\")\n",
    "print(\"Run with: nsys profile -w true -t cuda,nvtx,osrt,cudnn,cublas -s cpu --capture-range=cudaProfilerApi --stop-on-range-end=true --cudabacktrace=true -x true -o streamed_profile python streamed_main.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc51e9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
